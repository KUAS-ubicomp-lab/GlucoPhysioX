{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample code for calculating the Cross Entropies between HR and Temp for the first two subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pickle\n",
    "import EntropyHub as EH\n",
    "import gc \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_subjects = []\n",
    "\n",
    "first_subject_id = 1\n",
    "last_subject_id = 16 # We have 16 subjects in total, here just as an example I have only imported data from the first two subjects \n",
    "\n",
    "\n",
    "data_folder = 'new_data' \n",
    "folder_to_save = 'BVPACC' #save computed entropies\n",
    "\n",
    "#name of row\n",
    "XEntropy_signals = 'bvp+acc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = {} # Dictionary to store sampling frequency of each signal\n",
    "fs[\"acc\"] = 32\n",
    "fs[\"bvp\"] = 64\n",
    "fs[\"eda\"] = 4\n",
    "fs[\"temp\"] = 4\n",
    "fs[\"hr\"]  = 1\n",
    "\n",
    "epoch_seconds = 5 * 60 # 1 epoch is 5 minutes => 5min * 60 seconds/min = 300 seconds \n",
    "\n",
    "epoch_size = 300\n",
    "# e4_dfs_list = {' hr',' temp', ' eda', ' acc', ' bvp'} \n",
    "\n",
    "e4_dfs_list = {'bvp','acc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded segmented df for subject 1.\n",
      "Loaded segmented df for subject 2.\n",
      "Loaded segmented df for subject 3.\n",
      "Loaded segmented df for subject 4.\n",
      "Loaded segmented df for subject 5.\n",
      "Loaded segmented df for subject 6.\n",
      "Loaded segmented df for subject 7.\n",
      "Loaded segmented df for subject 8.\n",
      "Loaded segmented df for subject 9.\n",
      "Loaded segmented df for subject 10.\n",
      "Loaded segmented df for subject 11.\n",
      "Loaded segmented df for subject 12.\n",
      "Loaded segmented df for subject 13.\n",
      "Loaded segmented df for subject 14.\n",
      "Loaded segmented df for subject 15.\n",
      "Loaded segmented df for subject 16.\n"
     ]
    }
   ],
   "source": [
    "segmented_dfs = {}\n",
    "for subject_id in [id for id in range(first_subject_id, last_subject_id + 1) if id not in dropped_subjects]:\n",
    "    segmented_dfs[subject_id] = pd.read_pickle(f'{data_folder}/segmented_df_{subject_id}_5MIN.pkl') \n",
    "    print(f'Loaded segmented df for subject {subject_id}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertically concatenated all epochs for subject 1, acc.\n",
      "Vertically concatenated all epochs for subject 1, bvp.\n",
      "Vertically concatenated all epochs for subject 2, acc.\n",
      "Vertically concatenated all epochs for subject 2, bvp.\n",
      "Vertically concatenated all epochs for subject 3, acc.\n",
      "Vertically concatenated all epochs for subject 3, bvp.\n",
      "Vertically concatenated all epochs for subject 4, acc.\n",
      "Vertically concatenated all epochs for subject 4, bvp.\n",
      "Vertically concatenated all epochs for subject 5, acc.\n",
      "Vertically concatenated all epochs for subject 5, bvp.\n",
      "Vertically concatenated all epochs for subject 6, acc.\n",
      "Vertically concatenated all epochs for subject 6, bvp.\n",
      "Vertically concatenated all epochs for subject 7, acc.\n",
      "Vertically concatenated all epochs for subject 7, bvp.\n",
      "Vertically concatenated all epochs for subject 8, acc.\n",
      "Vertically concatenated all epochs for subject 8, bvp.\n",
      "Vertically concatenated all epochs for subject 9, acc.\n",
      "Vertically concatenated all epochs for subject 9, bvp.\n",
      "Vertically concatenated all epochs for subject 10, acc.\n",
      "Vertically concatenated all epochs for subject 10, bvp.\n",
      "Vertically concatenated all epochs for subject 11, acc.\n",
      "Vertically concatenated all epochs for subject 11, bvp.\n",
      "Vertically concatenated all epochs for subject 12, acc.\n",
      "Vertically concatenated all epochs for subject 12, bvp.\n",
      "Vertically concatenated all epochs for subject 13, acc.\n",
      "Vertically concatenated all epochs for subject 13, bvp.\n",
      "Vertically concatenated all epochs for subject 14, acc.\n",
      "Vertically concatenated all epochs for subject 14, bvp.\n",
      "Vertically concatenated all epochs for subject 15, acc.\n",
      "Vertically concatenated all epochs for subject 15, bvp.\n",
      "Vertically concatenated all epochs for subject 16, acc.\n",
      "Vertically concatenated all epochs for subject 16, bvp.\n"
     ]
    }
   ],
   "source": [
    "# Combine all epochs vertically to form one DataFrame\n",
    "\n",
    "concatenated_dfs = {}\n",
    "\n",
    "for subject_id in [id for id in range(first_subject_id, last_subject_id + 1) if id not in dropped_subjects]:\n",
    "    for signal_name in e4_dfs_list:\n",
    "        try:\n",
    "            concatenated_dfs[f'{subject_id}, {signal_name}'] = pd.concat(segmented_dfs[subject_id][f'{signal_name}_segments'].values, ignore_index=True)\n",
    "            print(f'Vertically concatenated all epochs for subject {subject_id}, {signal_name}.')\n",
    "        except Exception as e:\n",
    "            print(f'Error - {e} for {subject_id}, {signal_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Cross Entropy calculation, both signals must have the same length. Therefore, we need to resample it as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled concatenated df for subject 1, acc.\n",
      "Resampled concatenated df for subject 1, bvp.\n",
      "Resampled concatenated df for subject 2, acc.\n",
      "Resampled concatenated df for subject 2, bvp.\n",
      "Resampled concatenated df for subject 3, acc.\n",
      "Resampled concatenated df for subject 3, bvp.\n",
      "Resampled concatenated df for subject 4, acc.\n",
      "Resampled concatenated df for subject 4, bvp.\n",
      "Resampled concatenated df for subject 5, acc.\n",
      "Resampled concatenated df for subject 5, bvp.\n",
      "Resampled concatenated df for subject 6, acc.\n",
      "Resampled concatenated df for subject 6, bvp.\n",
      "Resampled concatenated df for subject 7, acc.\n",
      "Resampled concatenated df for subject 7, bvp.\n",
      "Resampled concatenated df for subject 8, acc.\n",
      "Resampled concatenated df for subject 8, bvp.\n",
      "Resampled concatenated df for subject 9, acc.\n",
      "Resampled concatenated df for subject 9, bvp.\n",
      "Resampled concatenated df for subject 10, acc.\n",
      "Resampled concatenated df for subject 10, bvp.\n",
      "Resampled concatenated df for subject 11, acc.\n",
      "Error - Unable to allocate 311. MiB for an array with shape (40742400,) and data type int64 for 11, bvp\n",
      "Resampled concatenated df for subject 12, acc.\n",
      "Error - Unable to allocate 236. MiB for an array with shape (30892800,) and data type int64 for 12, bvp\n",
      "Error - Unable to allocate 136. MiB for an array with shape (17808000,) and data type int64 for 13, acc\n",
      "Error - Unable to allocate 272. MiB for an array with shape (35616000,) and data type int64 for 13, bvp\n",
      "Error - Unable to allocate 112. MiB for an array with shape (14668800,) and data type int64 for 14, acc\n",
      "Error - Unable to allocate 224. MiB for an array with shape (29337600,) and data type int64 for 14, bvp\n",
      "Error - Unable to allocate 35.4 MiB for an array with shape (4646400,) and data type int64 for 15, acc\n",
      "Error - Unable to allocate 70.9 MiB for an array with shape (9292800,) and data type int64 for 15, bvp\n",
      "Error - Unable to allocate 96.7 MiB for an array with shape (12672000,) and data type int64 for 16, acc\n",
      "Error - Unable to allocate 193. MiB for an array with shape (25344000,) and data type int64 for 16, bvp\n"
     ]
    }
   ],
   "source": [
    "resampled_dfs = {}\n",
    "\n",
    "for subject_id in [id for id in range(first_subject_id, last_subject_id + 1) if id not in dropped_subjects]:\n",
    "    for signal_name in e4_dfs_list:\n",
    "        try:\n",
    "            resampled_dfs[f'{subject_id}, {signal_name}'] = concatenated_dfs[f'{subject_id}, {signal_name}'].groupby(concatenated_dfs[f'{subject_id}, {signal_name}'].index // fs[f'{signal_name}']).mean()\n",
    "            print(f'Resampled concatenated df for subject {subject_id}, {signal_name}.')\n",
    "        except Exception as e:\n",
    "            print(f'Error - {e} for {subject_id}, {signal_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Unable to allocate 3.02 MiB for an array with shape (396000, 1) and data type float64 for subject 3, acc.\n",
      "Error Unable to allocate 3.02 MiB for an array with shape (396000, 1) and data type float64 for subject 3, bvp.\n",
      "Error Unable to allocate 3.09 MiB for an array with shape (405000, 1) and data type float64 for subject 4, acc.\n",
      "Error Unable to allocate 3.09 MiB for an array with shape (405000, 1) and data type float64 for subject 4, bvp.\n",
      "Error Unable to allocate 5.43 MiB for an array with shape (711900, 1) and data type float64 for subject 5, acc.\n",
      "Error Unable to allocate 5.43 MiB for an array with shape (711900, 1) and data type float64 for subject 5, bvp.\n",
      "Error Unable to allocate 3.60 MiB for an array with shape (471600, 1) and data type float64 for subject 6, acc.\n",
      "Error Unable to allocate 3.60 MiB for an array with shape (471600, 1) and data type float64 for subject 6, bvp.\n",
      "Error Unable to allocate 4.32 MiB for an array with shape (565800, 1) and data type float64 for subject 7, acc.\n",
      "Error Unable to allocate 4.32 MiB for an array with shape (565800, 1) and data type float64 for subject 7, bvp.\n",
      "Error Unable to allocate 4.61 MiB for an array with shape (604800, 1) and data type float64 for subject 8, acc.\n",
      "Error Unable to allocate 4.61 MiB for an array with shape (604800, 1) and data type float64 for subject 8, bvp.\n",
      "Error Unable to allocate 4.69 MiB for an array with shape (614100, 1) and data type float64 for subject 9, acc.\n",
      "Error Unable to allocate 4.69 MiB for an array with shape (614100, 1) and data type float64 for subject 9, bvp.\n",
      "Error Unable to allocate 4.44 MiB for an array with shape (582300, 1) and data type float64 for subject 10, acc.\n",
      "Error Unable to allocate 4.44 MiB for an array with shape (582300, 1) and data type float64 for subject 10, bvp.\n",
      "Error Unable to allocate 4.86 MiB for an array with shape (636600, 1) and data type float64 for subject 11, acc.\n",
      "Error '11, bvp' for subject 11, bvp.\n",
      "Error Unable to allocate 3.68 MiB for an array with shape (482700, 1) and data type float64 for subject 12, acc.\n",
      "Error '12, bvp' for subject 12, bvp.\n",
      "Error '13, acc' for subject 13, acc.\n",
      "Error '13, bvp' for subject 13, bvp.\n",
      "Error '14, acc' for subject 14, acc.\n",
      "Error '14, bvp' for subject 14, bvp.\n",
      "Error '15, acc' for subject 15, acc.\n",
      "Error '15, bvp' for subject 15, bvp.\n",
      "Error '16, acc' for subject 16, acc.\n",
      "Error '16, bvp' for subject 16, bvp.\n"
     ]
    }
   ],
   "source": [
    "# The signal needs to be reshaped into an array of shape: (number_of_epochs, epoch_size)\n",
    "\n",
    "signal_reshaped = {}\n",
    "for subject_id in [id for id in range(first_subject_id, last_subject_id + 1) if id not in dropped_subjects]:\n",
    "    for signal_name in e4_dfs_list: \n",
    "            try:\n",
    "                signal_to_reshape = np.array(resampled_dfs[f'{subject_id}, {signal_name}'])\n",
    "                signal_reshaped[f'{subject_id}, {signal_name}'] = signal_to_reshape.reshape(int(len(signal_to_reshape) / epoch_size), epoch_size)\n",
    "            except Exception as e:\n",
    "                print(f'Error {e} for subject {subject_id}, {signal_name}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - Non-linear domain (Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonlinear_features_gc(sig1, sig2, signal_name=\"\"):\n",
    "  \n",
    "    feature_list = [\"XAp0\", \"XAp1\", \"XAp2\",\n",
    "    \"XCond1\", \"XCond2\",  \n",
    "    \"XDist\",\n",
    "    \"XFuzz1\",\"XFuzz2\",\n",
    "    \"XK2_1\",  \"XK2_2\", \n",
    "    \"XPermEn\",\n",
    "    \"XSamp0\",\"XSamp1\", \"XSamp2\",\n",
    "    \"XSpec\"]\n",
    "\n",
    "\n",
    "    for i in range(len(feature_list)):\n",
    "        feature_list[i] = signal_name + \"_\" + feature_list[i]\n",
    "    \n",
    "    aXAp0 = np.ones(len(sig1)) * np.nan\n",
    "    aXAp1 = np.ones(len(sig1)) * np.nan\n",
    "    aXAp2 = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aPhi = np.ones(len(sig1)) * np.nan\n",
    "    aPhi1 = np.ones(len(sig1)) * np.nan\n",
    "    aPhi2 = np.ones(len(sig1)) * np.nan\n",
    "    aPhi3 = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aXCond1 = np.ones(len(sig1)) * np.nan\n",
    "    aXCond2 = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aSEw = np.ones(len(sig1)) * np.nan\n",
    "    aSEz = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aXDist = np.ones(len(sig1)) * np.nan\n",
    "    aPpi = np.ones(len(sig1)) * np.nan\n",
    "    aPpi1 = np.ones(len(sig1)) * np.nan\n",
    "    aPpi2 = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aXFuzz1 = np.ones(len(sig1)) * np.nan\n",
    "    aXFuzz2 = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aPs1 = np.ones(len(sig1)) * np.nan\n",
    "    aPs1_1 = np.ones(len(sig1)) * np.nan\n",
    "    aPs1_2 = np.ones(len(sig1)) * np.nan\n",
    "    aPs2 = np.ones(len(sig1)) * np.nan\n",
    "    aPs2_1 = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aXK2_1 = np.ones(len(sig1)) * np.nan\n",
    "    aXK2_2 = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aCi = np.ones(len(sig1)) * np.nan\n",
    "    aCi1 = np.ones(len(sig1)) * np.nan\n",
    "    aCi2 = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aXPermEn = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aXSamp0 = np.ones(len(sig1)) * np.nan\n",
    "    aXSamp1 = np.ones(len(sig1)) * np.nan\n",
    "    aXSamp2 = np.ones(len(sig1)) * np.nan\n",
    "    aA = np.ones(len(sig1)) * np.nan\n",
    "    aA1 = np.ones(len(sig1)) * np.nan\n",
    "    aA2 = np.ones(len(sig1)) * np.nan\n",
    "    aB = np.ones(len(sig1)) * np.nan\n",
    "    aB1 = np.ones(len(sig1)) * np.nan\n",
    "    aB2 = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    aXSpec = np.ones(len(sig1)) * np.nan\n",
    "    aBandEn = np.ones(len(sig1)) * np.nan\n",
    "\n",
    "    for i in range(len(sig1)):\n",
    "        if (~np.isnan(sig1[i]).any() and ~np.isnan(sig2[i]).any()):\n",
    "            if (len(np.unique(sig1[i])) != 1 and len(np.unique(sig2[i])) != 1):\n",
    "\n",
    "                \n",
    "                #XApEn\n",
    "                try:\n",
    "                    XAp, Phi = EH.XApEn(sig1[i], sig2[i])\n",
    "                    aXAp0[i] = XAp[0]\n",
    "                    aXAp1[i] = XAp[1]\n",
    "                    aXAp2[i] = XAp[2]\n",
    "\n",
    "                    aPhi[i] = Phi[0]\n",
    "                    aPhi1[i] = Phi[1]\n",
    "                    aPhi2[i] = Phi[2]\n",
    "                    aPhi3[i] = Phi[3]\n",
    "                except Exception as e:\n",
    "                   print(f\"Error computing XApEn for index {i}: {e}\")\n",
    "\n",
    "                #XCondEn\n",
    "                    \n",
    "                try:\n",
    "                    XCond, SEw, SEz = EH.XCondEn(sig1[i], sig2[i])\n",
    "                    aXCond1[i] = XCond[0]\n",
    "                    aXCond2[i] = XCond[1]\n",
    "\n",
    "                    aSEw[i] = SEw\n",
    "                    aSEz[i] = SEz\n",
    "\n",
    "                except Exception as e:\n",
    "                   print(f\"Error computing XCond for index {i}: {e}\")\n",
    "        \n",
    "                #XDistEn\n",
    "                try:\n",
    "                    XDist, Ppi = EH.XDistEn(sig1[i], sig2[i])\n",
    "                    aXDist[i] = XDist\n",
    "\n",
    "                    aPpi[i] = Ppi[0]\n",
    "                    aPpi1[i] = Ppi[1]\n",
    "                    aPpi2[i] = Ppi[2]\n",
    "                except Exception as e:\n",
    "                   print(f\"Error computing XDistEn for index {i}: {e}\")\n",
    "\n",
    "                #XFuzzEn\n",
    "                try:\n",
    "                    XFuzz, Ps1, Ps2 = EH.XFuzzEn(sig1[i], sig2[i])\n",
    "                    aXFuzz1[i] = XFuzz[0]\n",
    "                    aXFuzz2[i] = XFuzz[1]\n",
    "\n",
    "                    aPs1[i] = Ps1[0]\n",
    "                    aPs1_1[i] = Ps1[1]\n",
    "                    aPs1_2[i] = Ps1[2]\n",
    "\n",
    "                    aPs2[i] = Ps2[0]\n",
    "                    aPs2_1[i] = Ps2[1]\n",
    "\n",
    "                except Exception as e:\n",
    "                        print(f\"Error computing XFuzzEn for index {i}: {e}\")\n",
    "\n",
    "                #XK2En\n",
    "                try:\n",
    "                    XK2, Ci = EH.XK2En(sig1[i], sig2[i])\n",
    "                    aXK2_1[i] = XK2[0]\n",
    "                    aXK2_2[i] = XK2[1]\n",
    "\n",
    "                    aCi[i] = Ci[0]\n",
    "                    aCi1[i] = Ci[1]\n",
    "                    aCi2[i] = Ci[2]\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error computing XK2En for index {i}: {e}\")\n",
    "\n",
    "                #XPermEn\n",
    "                try:\n",
    "                    aXPermEn[i] = EH.XPermEn(sig1[i], sig2[i])\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error computing XPermEn for index {i}: {e}\")\n",
    "\n",
    "                #XSampEn\n",
    "                try:\n",
    "                    XSamp, A, B = EH.XSampEn(sig1[i], sig2[i])\n",
    "                    aXSamp0[i] = XSamp[0]\n",
    "                    aXSamp1[i] = XSamp[1]\n",
    "                    aXSamp2[i] = XSamp[2]\n",
    "\n",
    "                    aA[i] = A[0]\n",
    "                    aA1[i] = A[1]\n",
    "                    aA2[i] = A[2]\n",
    "                    \n",
    "                    aB[i] = B[0]\n",
    "                    aB1[i] = B[1]\n",
    "                    aB2[i] = B[2]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error computing XSampEn for index {i}: {e}\")\n",
    "\n",
    "                #XSpecEn\n",
    "                try:\n",
    "                    XSpec, BandEn = EH.XSpecEn(sig1[i], sig2[i])\n",
    "                    aXSpec[i] = XSpec \n",
    "                    aBandEn[i] = BandEn\n",
    "                except Exception as e:\n",
    "                    print(f\"Error computing XSpecEn for index {i}: {e}\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    results = [aXAp0, aXAp1, aXAp2, \n",
    "    aXCond1, aXCond2, \n",
    "    aXDist,\n",
    "    aXFuzz1, aXFuzz2, \n",
    "    aXK2_1, aXK2_2, \n",
    "    aXPermEn,\n",
    "    aXSamp0, aXSamp1, aXSamp2, \n",
    "    aXSpec]\n",
    "    \n",
    "    return pd.DataFrame(np.array(results).T, columns=feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error '1,bvp' for 1, bvp+acc.\n",
      "!!!!!!!!!processed for subject 1!!!!!!!!!\n",
      "Error '2,bvp' for 2, bvp+acc.\n",
      "!!!!!!!!!processed for subject 2!!!!!!!!!\n",
      "Error '3,bvp' for 3, bvp+acc.\n",
      "!!!!!!!!!processed for subject 3!!!!!!!!!\n",
      "Error '4,bvp' for 4, bvp+acc.\n",
      "!!!!!!!!!processed for subject 4!!!!!!!!!\n",
      "Error '5,bvp' for 5, bvp+acc.\n",
      "!!!!!!!!!processed for subject 5!!!!!!!!!\n",
      "Error '6,bvp' for 6, bvp+acc.\n",
      "!!!!!!!!!processed for subject 6!!!!!!!!!\n",
      "Error '7,bvp' for 7, bvp+acc.\n",
      "!!!!!!!!!processed for subject 7!!!!!!!!!\n",
      "Error '8,bvp' for 8, bvp+acc.\n",
      "!!!!!!!!!processed for subject 8!!!!!!!!!\n",
      "Error '9,bvp' for 9, bvp+acc.\n",
      "!!!!!!!!!processed for subject 9!!!!!!!!!\n",
      "Error '10,bvp' for 10, bvp+acc.\n",
      "!!!!!!!!!processed for subject 10!!!!!!!!!\n",
      "Error '11,bvp' for 11, bvp+acc.\n",
      "!!!!!!!!!processed for subject 11!!!!!!!!!\n",
      "Error '12,bvp' for 12, bvp+acc.\n",
      "!!!!!!!!!processed for subject 12!!!!!!!!!\n",
      "Error '13,bvp' for 13, bvp+acc.\n",
      "!!!!!!!!!processed for subject 13!!!!!!!!!\n",
      "Error '14,bvp' for 14, bvp+acc.\n",
      "!!!!!!!!!processed for subject 14!!!!!!!!!\n",
      "Error '15,bvp' for 15, bvp+acc.\n",
      "!!!!!!!!!processed for subject 15!!!!!!!!!\n",
      "Error '16,bvp' for 16, bvp+acc.\n",
      "!!!!!!!!!processed for subject 16!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "non_linear_features = {}\n",
    "\n",
    "\n",
    "# Here, I have explicilty chosen the HR and Temp signals for calculating cross entropy.. \n",
    "# Later, you will need to calculate cross entropies for all possible combinations.. eg: HR and EDA, Temp and EDA etc\n",
    "for subject_id in [id for id in range(first_subject_id, last_subject_id + 1) if id not in dropped_subjects]:   \n",
    "    try:\n",
    "        non_linear_features[f'{subject_id}, {XEntropy_signals}'] = get_nonlinear_features_gc(signal_reshaped[f'{subject_id},bvp'], signal_reshaped[f'{subject_id},acc'], f'{XEntropy_signals}')\n",
    "    except Exception as e:\n",
    "        print(f'Error {e} for {subject_id}, {XEntropy_signals}.')\n",
    "    print(f'!!!!!!!!!processed for subject {subject_id}!!!!!!!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features():\n",
    "    for subject_id in [id for id in range(first_subject_id, last_subject_id + 1) if id not in dropped_subjects]:\n",
    "        #non_linear_features[f'{subject_id}, {XEntropy_signals}'].to_pickle(f'{folder_to_save}/{XEntropy_signals}_{subject_id}.pkl')\n",
    "        non_linear_features[f'{subject_id}, {XEntropy_signals}'].to_csv(f'{folder_to_save}/{XEntropy_signals}_{subject_id}.csv')\n",
    "        print(f'Saved XApEn for subject {subject_id}. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1, bvp+acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19632\\1433931006.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msave_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19632\\4036975823.py\u001b[0m in \u001b[0;36msave_features\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msubject_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_subject_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_subject_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdropped_subjects\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;31m#non_linear_features[f'{subject_id}, {XEntropy_signals}'].to_pickle(f'{folder_to_save}/{XEntropy_signals}_{subject_id}.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mnon_linear_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'{subject_id}, {XEntropy_signals}'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{folder_to_save}/{XEntropy_signals}_{subject_id}.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Saved XApEn for subject {subject_id}. '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '1, bvp+acc'"
     ]
    }
   ],
   "source": [
    "save_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
